{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2WDFMWsRh93"
      },
      "outputs": [],
      "source": [
        "from transformers import  AutoTokenizer\n",
        "import warnings\n",
        "!pip install datasets\n",
        "warnings.simplefilter(\"ignore\")\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math,copy,re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import gc\n",
        "import spacy\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchtext.vocab import vocab\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic=True\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embed_layer = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.embed_layer(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, max_seq_len, embed_dim):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        pe = torch.zeros((self.max_seq_len, self.embed_dim))\n",
        "\n",
        "        for pos in range(self.max_seq_len):\n",
        "            for i in range(0, self.embed_dim, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000**(i/self.embed_dim)))\n",
        "                pe[pos, i+1] = math.cos(pos / (10000**(i/self.embed_dim)))\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        inp = inp*math.sqrt(self.embed_dim)\n",
        "        seq_len = inp.size(1)\n",
        "        inp = inp + torch.autograd.Variable(self.pe[:, :seq_len], requires_grad=False)\n",
        "        return inp\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim=512, n_heads=8):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.head_dim = int(self.embed_dim/self.n_heads)\n",
        "\n",
        "        self.query_matrix = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.key_matrix = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.value_matrix = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "\n",
        "        self.out = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "\n",
        "    def forward(self, key, query, value, mask=None):\n",
        "        batch_size = key.size(0)\n",
        "        seq_len = key.size(1)\n",
        "\n",
        "        seq_len_query = query.size(1)\n",
        "\n",
        "        key = key.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
        "        query = query.view(batch_size, seq_len_query, self.n_heads, self.head_dim)\n",
        "        value = value.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
        "\n",
        "        k = self.key_matrix(key)\n",
        "        q = self.query_matrix(query)\n",
        "        v = self.value_matrix(value)\n",
        "\n",
        "        k = k.transpose(1,2)\n",
        "        q = q.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "\n",
        "        k_adj = k.transpose(-1,-2)\n",
        "\n",
        "        # prdt = torch.einsum(\"bhqd,bhdk->bhqk\", q, k_adj)\n",
        "        prdt = torch.matmul(q, k_adj)\n",
        "\n",
        "        if mask is not None:\n",
        "            prdt = prdt.masked_fill(mask==0, float(\"-1e20\"))\n",
        "\n",
        "        prdt = prdt/math.sqrt(self.embed_dim)\n",
        "        prdt = F.softmax(prdt, dim=-1)\n",
        "\n",
        "        # attention = torch.einsum(\"bhqk,bhkd->bhqd\", prdt, v)\n",
        "        attention = torch.matmul(prdt, v)\n",
        "\n",
        "        concat = attention.transpose(1,2).contiguous().view(batch_size, seq_len_query, self.head_dim*self.n_heads)\n",
        "\n",
        "        out = self.out(concat)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim=512, n_heads=8, expansion_factor=4):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.expansion_factor = expansion_factor\n",
        "        self.multiheadattention = MultiHeadAttention(self.embed_dim, self.n_heads)\n",
        "        self.norm1 = nn.LayerNorm(self.embed_dim)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(self.embed_dim, self.embed_dim*self.expansion_factor),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.embed_dim*self.expansion_factor, self.embed_dim)\n",
        "            )\n",
        "        self.norm2 = nn.LayerNorm(self.embed_dim)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "\n",
        "\n",
        "    def forward(self, key, query, value, mask=None):\n",
        "        attention_out = self.multiheadattention(key, query, value, mask)\n",
        "        attention_residual_out = attention_out + query\n",
        "        norm1_out = self.dropout1(self.norm1(attention_residual_out))\n",
        "\n",
        "        feed_forward_out = self.feed_forward(norm1_out)\n",
        "        feed_forward_residual_out = feed_forward_out + norm1_out\n",
        "        norm2_out = self.dropout2(self.norm2(feed_forward_residual_out))\n",
        "\n",
        "        return norm2_out\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, max_seq_len, vocab_size, embed_size=512, num_layers=6, n_heads=8, expansion_factor=4):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.embedding_layer = Embeddings(vocab_size, embed_size)\n",
        "        self.positional_embeddings = PositionalEmbedding(max_seq_len, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(embed_size, n_heads, expansion_factor) for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        embed = self.embedding_layer(x)\n",
        "        out = self.positional_embeddings(embed)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out, mask)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim=512, n_heads=8, expansion_factor=4):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.expansion_factor = expansion_factor\n",
        "\n",
        "        self.transformer_block = TransformerBlock(embed_dim, n_heads, expansion_factor)\n",
        "        self.attention = MultiHeadAttention(embed_dim, n_heads)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, key, value, x, tgt_mask, src_mask=None):\n",
        "        attention = self.attention(x, x, x, tgt_mask)\n",
        "        query = self.dropout(self.norm(attention + x))\n",
        "        out = self.transformer_block(key, query, value, src_mask)\n",
        "        return out\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, max_seq_len, target_vocab_size, embed_dim=512, num_layers=6, expansion_factor=4, n_heads=8):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "\n",
        "        self.word_embedding = Embeddings(target_vocab_size, embed_dim)\n",
        "        self.position_embedding = PositionalEmbedding(max_seq_len, embed_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                DecoderBlock(embed_dim, expansion_factor=expansion_factor, n_heads=n_heads)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_dim, target_vocab_size)\n",
        "\n",
        "    def forward(self, x, enc_out, tgt_mask, src_mask=None):\n",
        "        embed = self.word_embedding(x)\n",
        "        x = self.position_embedding(embed)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(enc_out, enc_out, x, tgt_mask, src_mask)\n",
        "\n",
        "        logits = self.fc_out(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embed_dim, src_vocab_size, target_vocab_size, max_seq_length, num_layers=6, expansion_factor=4, n_heads=8, device='cpu'):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.src_pad_idx = -1\n",
        "        self.tgt_pad_idx = -1\n",
        "        self.device = device\n",
        "\n",
        "        self.encoder = TransformerEncoder(max_seq_length,\n",
        "                                          src_vocab_size,\n",
        "                                          embed_dim,\n",
        "                                          num_layers=num_layers,\n",
        "                                          expansion_factor=expansion_factor,\n",
        "                                          n_heads=n_heads)\n",
        "\n",
        "        self.decoder = TransformerDecoder(max_seq_length,\n",
        "                                          target_vocab_size,\n",
        "                                          embed_dim,\n",
        "                                          num_layers=num_layers,\n",
        "                                          expansion_factor=expansion_factor,\n",
        "                                          n_heads=n_heads)\n",
        "\n",
        "\n",
        "    def make_tgt_mask(self, tgt):\n",
        "        batch_size, tgt_len = tgt.shape\n",
        "        tgt_mask = torch.tril(torch.ones((tgt_len, tgt_len))).expand(\n",
        "            batch_size, 1, tgt_len, tgt_len\n",
        "        ).bool()\n",
        "        tgt_pad_mask = (tgt.cpu() != self.tgt_pad_idx).unsqueeze(1).unsqueeze(2).bool()\n",
        "        tgt_mask = tgt_mask & tgt_pad_mask\n",
        "        return tgt_mask.to(self.device)\n",
        "\n",
        "    def make_pad_mask(self, inp, pad_idx):\n",
        "        mask = (inp != pad_idx).unsqueeze(1).unsqueeze(2).bool()\n",
        "        return mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        tgt_mask = self.make_tgt_mask(tgt)\n",
        "        src_mask = self.make_pad_mask(src, self.src_pad_idx)\n",
        "        enc_out = self.encoder(src)\n",
        "        outputs = self.decoder(tgt, enc_out, tgt_mask, src_mask)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "XJd8Ecb5xxML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "SBPz4qZU4FPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_input(x):\n",
        "  x = str(x)\n",
        "  return x"
      ],
      "metadata": {
        "id": "vkh_6BPH2cRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('/content/drive/MyDrive/internal/mpng_cust/units/all_mapped_units.csv')\n",
        "data['source_code'] = data['source_code'].apply(clean_input)\n",
        "data['ln'] = data['source_code'].apply(lambda x: len(list(x.encode('ascii',errors='ignore'))))\n",
        "data = data[~data['ln'] < 60]\n",
        "# data['source_code'] = data['source_code'].apply(lambda x: clean_input(x))\n",
        "# data['concept_name'] = data['concept_name'].apply(lambda x: clean_input(x))"
      ],
      "metadata": {
        "id": "hmBNiJjTS5Mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail()"
      ],
      "metadata": {
        "id": "5oLu6Acz5C92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test = train_test_split(\n",
        "    data,\n",
        "    test_size=0.15,\n",
        "    random_state=42\n",
        "    )\n",
        "X_train['source_code'] = X_train['source_code'].apply(lambda x: str(x))\n",
        "X_train['concept_name'] = X_train['concept_name'].apply(lambda x: str(x))"
      ],
      "metadata": {
        "id": "IUYA81guWqPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets.dataset_dict import DatasetDict\n",
        "from datasets import Dataset\n",
        "\n",
        "dat = {'train':Dataset.from_dict({'label':X_train.concept_name,'text':X_train.source_code}),\n",
        "       'test':Dataset.from_dict({'label':X_test.concept_name,'text':X_test.source_code})\n",
        "     }"
      ],
      "metadata": {
        "id": "gtzNNLJPTsDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz\n",
        "spacy_eng = spacy.load(\"en_core_sci_sm\")\n",
        "def tokenizer_eng(text):\n",
        "    return [tok.text for tok in spacy_eng.tokenizer(text)]"
      ],
      "metadata": {
        "id": "ZV_btM7910Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dat1  = DatasetDict(dat)\n",
        "train, test = dat1['train'], dat1['test']"
      ],
      "metadata": {
        "id": "F8gr9_a3YcHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sou_counter = Counter()\n",
        "tar_counter = Counter()\n",
        "for data in tqdm(train):\n",
        "    sou_counter.update(list(data['text']))\n",
        "    tar_counter.update(tokenizer_eng(data['label'].lower()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAwdAvf_Xi9X",
        "outputId": "ea547051-1bd2-4232-da7c-db42245744b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7613/7613 [00:00<00:00, 11491.69it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r-HTCLZtBZWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "He3LY0Qhb_L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sou_vocab = vocab(sou_counter, min_freq=4, specials=(\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"))\n",
        "tar_vocab = vocab(tar_counter, min_freq=4, specials=(\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"))\n",
        "sou_vocab.set_default_index(sou_vocab[\"<unk>\"])\n",
        "tar_vocab.set_default_index(tar_vocab[\"<unk>\"])"
      ],
      "metadata": {
        "id": "3L8c370kZWhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Size of Source Vocab : {len(sou_vocab)}\\n Size of Target Vocab : {len(tar_vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8dMVo8XvUpD",
        "outputId": "ff5f3f9e-b5b5-484a-a120-f57f832e9aed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of Source Vocab : 96\n",
            " Size of Target Vocab : 241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_transform_tar = lambda x: [tar_vocab['<sos>']] + [tar_vocab[token.lower()] for token in tokenizer_eng(x)] + [tar_vocab['<eos>']]\n",
        "text_transform_sou = lambda x: [sou_vocab['<sos>']] + [sou_vocab[token] for token in list(x)] + [sou_vocab['<eos>']]"
      ],
      "metadata": {
        "id": "kPxCa6CJvVqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    src_list, tgt_list = [], []\n",
        "    for data in batch:\n",
        "        src_list.append(torch.tensor(text_transform_sou(data['text'])))\n",
        "        tgt_list.append(torch.tensor(text_transform_tar(data['label'])))\n",
        "    src_list = pad_sequence(src_list, padding_value=sou_vocab['<pad>']).T\n",
        "    tgt_list = pad_sequence(tgt_list, padding_value=tar_vocab['<pad>']).T\n",
        "    inp = {\n",
        "        \"src\": src_list,\n",
        "        \"tgt\": tgt_list\n",
        "    }\n",
        "\n",
        "    return inp"
      ],
      "metadata": {
        "id": "dzX4SoNxvn_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 28\n",
        "batch_size = 16\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 0.001\n",
        "writer = SummaryWriter(f\"runs/loss\")\n",
        "\n",
        "train_dataloader = DataLoader(train,\n",
        "                              collate_fn=collate_batch,\n",
        "                              shuffle=True,\n",
        "                              batch_size=batch_size,\n",
        "                              pin_memory=True)\n",
        "test_dataloader = DataLoader(test,\n",
        "                              collate_fn=collate_batch,\n",
        "                              shuffle=False,\n",
        "                              batch_size=batch_size,\n",
        "                              pin_memory=True)"
      ],
      "metadata": {
        "id": "K1loJ3qbxnmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "transformer_model = Transformer(embed_dim=512,\n",
        "                                src_vocab_size=len(sou_vocab),\n",
        "                                target_vocab_size=len(tar_vocab),\n",
        "                                max_seq_length=105,\n",
        "                                num_layers=6,\n",
        "                                expansion_factor=4,\n",
        "                                n_heads=8,\n",
        "                                device=device)\n",
        "transformer_model.src_pad_idx = sou_vocab['<pad>']\n",
        "transformer_model.tgt_pad_idx = tar_vocab['<pad>']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import math\n",
        "total_steps = num_epochs*math.ceil(len(train)/batch_size)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
        "                                               max_lr=learning_rate,\n",
        "                                               total_steps=total_steps,\n",
        "                                               pct_start=0.33,\n",
        "                                               div_factor=1e3,\n",
        "                                               final_div_factor=1e2)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tar_vocab['<pad>'])"
      ],
      "metadata": {
        "id": "jPytF6SYx2H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_model = transformer_model.to(device)"
      ],
      "metadata": {
        "id": "rN5NcX2px7d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_seq_beam_search(model, src, device, k=2, max_len=105):\n",
        "    model.eval()\n",
        "\n",
        "    src_mask = model.make_pad_mask(src, model.src_pad_idx)\n",
        "    with torch.no_grad():\n",
        "        enc_out = model.encoder(src, src_mask)\n",
        "\n",
        "    # beam search\n",
        "\n",
        "    candidates = [(torch.LongTensor([tar_vocab['<sos>']]), 0.0)]\n",
        "\n",
        "    final_translations = []\n",
        "\n",
        "    for a in range(max_len):\n",
        "\n",
        "        input_batch = torch.concat([c[0].unsqueeze(0) for c in candidates], dim=0).to(device)\n",
        "\n",
        "        if a>0:\n",
        "            enc_out_repeat = enc_out.repeat(input_batch.shape[0], 1, 1)\n",
        "        else:\n",
        "            enc_out_repeat = enc_out\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.decoder(input_batch, enc_out_repeat, model.make_tgt_mask(input_batch), src_mask).detach().cpu()\n",
        "        output[:, :, :2] = float(\"-1e20\")\n",
        "        output = output[:, -1, :]\n",
        "        output = F.log_softmax(output, dim=-1)\n",
        "\n",
        "\n",
        "        topk_output = torch.topk(output, k, dim=-1)\n",
        "        topk_tokens = topk_output.indices\n",
        "        topk_scores = topk_output.values\n",
        "\n",
        "\n",
        "        new_seq = torch.concat([torch.concat([torch.vstack([c[0] for _ in range(k)]), topk_tokens[i].reshape(-1,1)], dim=-1) for i,c in enumerate(candidates)], dim=0)\n",
        "        new_scores = torch.concat([c[1] + topk_scores[i] for i,c in enumerate(candidates)], dim=0)\n",
        "\n",
        "\n",
        "        topk_new = torch.topk(new_scores, k=k).indices.tolist()\n",
        "\n",
        "        new_candidates = []\n",
        "\n",
        "        for i in range(k):\n",
        "            if new_seq[topk_new[i]][-1] == tar_vocab[\"<eos>\"] or a==max_len-1:\n",
        "                final_translations.append((new_seq[topk_new[i]].tolist(), int(new_scores[topk_new[i]])))\n",
        "            else:\n",
        "                new_candidate = (new_seq[topk_new[i]], new_scores[topk_new[i]])\n",
        "                new_candidates.append(new_candidate)\n",
        "\n",
        "\n",
        "        if len(new_candidates) > 0:\n",
        "            candidates = new_candidates\n",
        "        else:\n",
        "            break\n",
        "\n",
        "\n",
        "    return final_translations\n",
        "def translate_seq(model, src, device, max_len=105):\n",
        "    model.eval()\n",
        "    src_mask = model.make_pad_mask(src, model.src_pad_idx)\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src, src_mask)\n",
        "    tgt_indexes = [tar_vocab[\"<sos>\"]]\n",
        "    for i in range(max_len):\n",
        "        tgt_tensor = torch.LongTensor(tgt_indexes).unsqueeze(0).to(device)\n",
        "        tgt_mask = model.make_tgt_mask(tgt_tensor)\n",
        "        with torch.no_grad():\n",
        "            output = model.decoder(tgt_tensor, enc_src, tgt_mask, src_mask)\n",
        "        output[:, :, :2] = float(\"-1e20\")  # cannot predict <unk>, <pad> token\n",
        "        output = output[:, -1, :] # pick the last token\n",
        "        output = F.softmax(output, dim=-1)\n",
        "        pred_token = output.argmax(-1).item()\n",
        "        tgt_indexes.append(pred_token)\n",
        "        if pred_token == tar_vocab[\"<eos>\"]:\n",
        "            break\n",
        "    return tgt_indexes\n",
        "\n",
        "\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "class AvgMeter:\n",
        "    def __init__(self, name=\"Metric\"):\n",
        "        self.name = name\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.avg, self.sum, self.count = [0]*3\n",
        "\n",
        "    def update(self, val, count=1):\n",
        "        self.count += count\n",
        "        self.sum += val * count\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __repr__(self):\n",
        "        text = f\"{self.name}: {self.avg:.4f}\"\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "F91M_ZDbziZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### training ###\n",
        "step = 0\n",
        "for epoch in range(1, num_epochs+1):\n",
        "\n",
        "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "\n",
        "    checkpoint = {\"state_dict\": transformer_model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
        "    torch.save(checkpoint, \"/content/drive/MyDrive/internal/mpng_cust/units/my_checkpoint_2.pth.tar\")\n",
        "\n",
        "    loss_meter = AvgMeter()\n",
        "    transformer_model.train()\n",
        "\n",
        "    bar = tqdm(train_dataloader, total=math.ceil(len(train)/batch_size))\n",
        "\n",
        "    for idx, data in enumerate(bar):\n",
        "\n",
        "        source_ = data[\"src\"].to(device)\n",
        "        target_ = data[\"tgt\"].to(device)\n",
        "\n",
        "        count = source_.shape[0]\n",
        "\n",
        "        output = transformer_model(source_, target_[:,:-1])\n",
        "\n",
        "        output = output.reshape(-1, output.shape[2])\n",
        "        target_ = target_[:, 1:]\n",
        "        target_ = target_.reshape(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, target_)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), max_norm=1)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
        "        step += 1\n",
        "\n",
        "        loss_meter.update(loss.item(), count)\n",
        "        bar.set_postfix(loss=loss_meter.avg, lr=get_lr(optimizer), step=step)\n",
        "\n",
        "    # Example Generation (Greedy Decode)\n",
        "    ex = test[random.randint(0, len(test))]\n",
        "    sentence = ex['text']\n",
        "    src_indexes = torch.tensor(text_transform_sou(sentence)).unsqueeze(0).to(device)\n",
        "    mapped_sentence_idx = translate_seq(transformer_model, src_indexes, device=device, max_len=30)\n",
        "    mapped_sentence = [tar_vocab.get_itos()[i] for i in mapped_sentence_idx]\n",
        "    print(f\"\\nExample source unit info: \\n{sentence}\\n\")\n",
        "    print(f\"Original UCUM mapping : \\n{ex['label']}\\n\" )\n",
        "    print(f\"Generated by model UCUM mapping : \\n{' '.join(mapped_sentence[1:-1])}\\n\")\n",
        "\n",
        "    del src_indexes, ex, sentence, mapped_sentence_idx, mapped_sentence, checkpoint\n",
        "    torch.cuda.empty_cache()\n",
        "    _ = gc.collect()\n"
      ],
      "metadata": {
        "id": "xd43KBs7zlcW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}