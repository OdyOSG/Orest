{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "!pip install datasets\n",
        "warnings.simplefilter(\"ignore\")\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math,copy,re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import gc\n",
        "import spacy\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchtext.vocab import vocab\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "print(torch.__version__)\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic=True\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embed_layer = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.embed_layer(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, max_seq_len, embed_dim):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        pe = torch.zeros((self.max_seq_len, self.embed_dim))\n",
        "\n",
        "        for pos in range(self.max_seq_len):\n",
        "            for i in range(0, self.embed_dim, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000**(i/self.embed_dim)))\n",
        "                pe[pos, i+1] = math.cos(pos / (10000**(i/self.embed_dim)))\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        inp = inp*math.sqrt(self.embed_dim)\n",
        "        seq_len = inp.size(1)\n",
        "        inp = inp + torch.autograd.Variable(self.pe[:, :seq_len], requires_grad=False)\n",
        "        return inp\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim=512, n_heads=8):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.head_dim = int(self.embed_dim/self.n_heads)\n",
        "\n",
        "        self.query_matrix = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.key_matrix = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.value_matrix = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "\n",
        "        self.out = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "\n",
        "    def forward(self, key, query, value, mask=None):\n",
        "        batch_size = key.size(0)\n",
        "        seq_len = key.size(1)\n",
        "\n",
        "        seq_len_query = query.size(1)\n",
        "\n",
        "        key = key.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
        "        query = query.view(batch_size, seq_len_query, self.n_heads, self.head_dim)\n",
        "        value = value.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
        "\n",
        "        k = self.key_matrix(key)\n",
        "        q = self.query_matrix(query)\n",
        "        v = self.value_matrix(value)\n",
        "\n",
        "        k = k.transpose(1,2)\n",
        "        q = q.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "\n",
        "        k_adj = k.transpose(-1,-2)\n",
        "\n",
        "        # prdt = torch.einsum(\"bhqd,bhdk->bhqk\", q, k_adj)\n",
        "        prdt = torch.matmul(q, k_adj)\n",
        "\n",
        "        if mask is not None:\n",
        "            prdt = prdt.masked_fill(mask==0, float(\"-1e20\"))\n",
        "\n",
        "        prdt = prdt/math.sqrt(self.embed_dim)\n",
        "        prdt = F.softmax(prdt, dim=-1)\n",
        "\n",
        "        # attention = torch.einsum(\"bhqk,bhkd->bhqd\", prdt, v)\n",
        "        attention = torch.matmul(prdt, v)\n",
        "\n",
        "        concat = attention.transpose(1,2).contiguous().view(batch_size, seq_len_query, self.head_dim*self.n_heads)\n",
        "\n",
        "        out = self.out(concat)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim=512, n_heads=8, expansion_factor=4):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.expansion_factor = expansion_factor\n",
        "\n",
        "        self.multiheadattention = MultiHeadAttention(self.embed_dim, self.n_heads)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(self.embed_dim)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(self.embed_dim, self.embed_dim*self.expansion_factor),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.embed_dim*self.expansion_factor, self.embed_dim)\n",
        "            )\n",
        "        self.norm2 = nn.LayerNorm(self.embed_dim)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "\n",
        "\n",
        "    def forward(self, key, query, value, mask=None):\n",
        "        attention_out = self.multiheadattention(key, query, value, mask)\n",
        "        attention_residual_out = attention_out + query\n",
        "        norm1_out = self.dropout1(self.norm1(attention_residual_out))\n",
        "\n",
        "        feed_forward_out = self.feed_forward(norm1_out)\n",
        "        feed_forward_residual_out = feed_forward_out + norm1_out\n",
        "        norm2_out = self.dropout2(self.norm2(feed_forward_residual_out))\n",
        "\n",
        "        return norm2_out\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, max_seq_len, vocab_size, embed_size=512, num_layers=6, n_heads=8, expansion_factor=4):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.embedding_layer = Embeddings(vocab_size, embed_size)\n",
        "        self.positional_embeddings = PositionalEmbedding(max_seq_len, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(embed_size, n_heads, expansion_factor) for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        embed = self.embedding_layer(x)\n",
        "        out = self.positional_embeddings(embed)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out, mask)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim=512, n_heads=8, expansion_factor=4):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.expansion_factor = expansion_factor\n",
        "\n",
        "        self.transformer_block = TransformerBlock(embed_dim, n_heads, expansion_factor)\n",
        "        self.attention = MultiHeadAttention(embed_dim, n_heads)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, key, value, x, tgt_mask, src_mask=None):\n",
        "        attention = self.attention(x, x, x, tgt_mask)\n",
        "        query = self.dropout(self.norm(attention + x))\n",
        "        out = self.transformer_block(key, query, value, src_mask)\n",
        "        return out\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, max_seq_len, target_vocab_size, embed_dim=512, num_layers=6, expansion_factor=4, n_heads=8):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "\n",
        "        self.word_embedding = Embeddings(target_vocab_size, embed_dim)\n",
        "        self.position_embedding = PositionalEmbedding(max_seq_len, embed_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                DecoderBlock(embed_dim, expansion_factor=expansion_factor, n_heads=n_heads)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_dim, target_vocab_size)\n",
        "\n",
        "    def forward(self, x, enc_out, tgt_mask, src_mask=None):\n",
        "        embed = self.word_embedding(x)\n",
        "        x = self.position_embedding(embed)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(enc_out, enc_out, x, tgt_mask, src_mask)\n",
        "\n",
        "        logits = self.fc_out(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embed_dim, src_vocab_size, target_vocab_size, max_seq_length, num_layers=6, expansion_factor=4, n_heads=8, device='cpu'):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.src_pad_idx = -1\n",
        "        self.tgt_pad_idx = -1\n",
        "        self.device = device\n",
        "\n",
        "        self.encoder = TransformerEncoder(max_seq_length,\n",
        "                                          src_vocab_size,\n",
        "                                          embed_dim,\n",
        "                                          num_layers=num_layers,\n",
        "                                          expansion_factor=expansion_factor,\n",
        "                                          n_heads=n_heads)\n",
        "\n",
        "        self.decoder = TransformerDecoder(max_seq_length,\n",
        "                                          target_vocab_size,\n",
        "                                          embed_dim,\n",
        "                                          num_layers=num_layers,\n",
        "                                          expansion_factor=expansion_factor,\n",
        "                                          n_heads=n_heads)\n",
        "\n",
        "\n",
        "    def make_tgt_mask(self, tgt):\n",
        "        batch_size, tgt_len = tgt.shape\n",
        "        tgt_mask = torch.tril(torch.ones((tgt_len, tgt_len))).expand(\n",
        "            batch_size, 1, tgt_len, tgt_len\n",
        "        ).bool()\n",
        "        tgt_pad_mask = (tgt.cpu() != self.tgt_pad_idx).unsqueeze(1).unsqueeze(2).bool()\n",
        "        tgt_mask = tgt_mask & tgt_pad_mask\n",
        "        return tgt_mask.to(self.device)\n",
        "\n",
        "    def make_pad_mask(self, inp, pad_idx):\n",
        "        mask = (inp != pad_idx).unsqueeze(1).unsqueeze(2).bool()\n",
        "        return mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        tgt_mask = self.make_tgt_mask(tgt)\n",
        "        src_mask = self.make_pad_mask(src, self.src_pad_idx)\n",
        "        enc_out = self.encoder(src)\n",
        "        outputs = self.decoder(tgt, enc_out, tgt_mask, src_mask)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHMuREcKEH3w",
        "outputId": "997f3df0-3116-4ed3-f6bc-af4fb2c9cd04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.16.1 dill-0.3.7 multiprocess-0.70.15\n",
            "2.1.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz\n",
        "spacy_eng = spacy.load(\"en_core_sci_sm\")"
      ],
      "metadata": {
        "id": "ciz_Dhe6F84g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d7bf315f-7639-4814-e3f5-b81c77447796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz\n",
            "  Downloading https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting spacy<3.5.0,>=3.4.1 (from en-core-sci-sm==0.5.1)\n",
            "  Downloading spacy-3.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (8.1.12)\n",
            "Collecting wasabi<1.1.0,>=0.9.1 (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1)\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2.0.10)\n",
            "Collecting typer<0.8.0,>=0.3.0 (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1)\n",
            "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (1.10.14)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (3.3.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2.1.4)\n",
            "Building wheels for collected packages: en-core-sci-sm\n",
            "  Building wheel for en-core-sci-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-sci-sm: filename=en_core_sci_sm-0.5.1-py3-none-any.whl size=15870854 sha256=3fc8fb8e973149c4d2b202ce622c5c12335df8d166981d44da9021d97f138388\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/8f/33/4c20916692704167c1d78c3f11e7deff5b7c2ba4875eacf648\n",
            "Successfully built en-core-sci-sm\n",
            "Installing collected packages: wasabi, typer, spacy, en-core-sci-sm\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.2\n",
            "    Uninstalling wasabi-1.1.2:\n",
            "      Successfully uninstalled wasabi-1.1.2\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.0\n",
            "    Uninstalling typer-0.9.0:\n",
            "      Successfully uninstalled typer-0.9.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.6.1\n",
            "    Uninstalling spacy-3.6.1:\n",
            "      Successfully uninstalled spacy-3.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "en-core-web-sm 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.4.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed en-core-sci-sm-0.5.1 spacy-3.4.4 typer-0.7.0 wasabi-0.10.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "spacy",
                  "wasabi"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_gdrive_csv = '/content/drive/MyDrive/internal/mpng_cust/conditions_mapped_cons2.csv'"
      ],
      "metadata": {
        "id": "RMvW0tADOSxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_input(x):\n",
        "  stop_words = ['noc', 'nos', 'unknown_unit', 'comment', 'due', 'unspecified',\n",
        "              'finding', 'null', 'none', 'unknown', 'xamfx', 'xamfc', 'histology',\n",
        "                'if', 'in', 'org', 'xaxcg', 'xabzh', 'xaivs', 'xanwb']\n",
        "  x = re.sub('[^\\w\\s]',' ', x.lower())\n",
        "  result = ' '.join([word for word in x.split() if word not in (stop_words)])\n",
        "  return result"
      ],
      "metadata": {
        "id": "mV9G0qZSKVQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "id": "DejqhTdr5-24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(path_to_gdrive_csv)\n",
        "df.TARGET_CONCEPT_NAME = df['TARGET_CONCEPT_NAME'].apply(lambda x: x.lower())"
      ],
      "metadata": {
        "id": "ZESG_KUNM8Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.SOURCE_CODE = df['SOURCE_CODE'].apply(clean_input)"
      ],
      "metadata": {
        "id": "0azDTZIlNAyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[~df['SOURCE_CODE'].str.contains(\"no |not \")]"
      ],
      "metadata": {
        "id": "hFZ5dUfyQgrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trash_to_exclude = list(df[df['SOURCE_CODE'].str.startswith('xa')]['SOURCE_CODE'])"
      ],
      "metadata": {
        "id": "b9d8Y0eA0Tj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trash_to_exclude"
      ],
      "metadata": {
        "id": "mAFXMGTvF9P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(7): trash_to_exclude.pop(0)"
      ],
      "metadata": {
        "id": "uiM6job20_Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[~df['SOURCE_CODE'].isin(trash_to_exclude)]"
      ],
      "metadata": {
        "id": "vp3asVdD1ipq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBO3Xz_TM4ks",
        "outputId": "dacbccdf-1158-4ff4-d359-58840cbaa4aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(116156, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test = train_test_split(df,test_size=0.15, random_state=42)"
      ],
      "metadata": {
        "id": "hfM4Fsc36Idu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.to_csv('test.csv')"
      ],
      "metadata": {
        "id": "o0GA47DI6Jnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test = train_test_split(df,test_size=0.15, random_state=42)\n",
        "spacy_eng = spacy.load(\"en_core_sci_sm\")\n",
        "\n",
        "\n",
        "from datasets.dataset_dict import DatasetDict\n",
        "from datasets import Dataset\n",
        "\n",
        "dat = {'train':Dataset.from_dict({'label':X_train.TARGET_CONCEPT_NAME,'text':X_train.SOURCE_CODE}),\n",
        "       'test':Dataset.from_dict({'label':X_test.TARGET_CONCEPT_NAME,'text':X_test.SOURCE_CODE})\n",
        "     }\n",
        "\n",
        "dat1  = DatasetDict(dat)\n",
        "train, test = dat1['train'], dat1['test']\n",
        "\n",
        "\n",
        "def tokenizer_eng(text):\n",
        "    return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "sou_counter = Counter()\n",
        "tar_counter = Counter()\n",
        "for data in tqdm(train):\n",
        "    sou_counter.update(tokenizer_eng(data['text'].lower()))\n",
        "    tar_counter.update(tokenizer_eng(data['label'].lower()))\n",
        "\n",
        "sou_vocab = vocab(sou_counter, min_freq=2, specials=(\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"))\n",
        "tar_vocab = vocab(tar_counter, min_freq=2, specials=(\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"))"
      ],
      "metadata": {
        "id": "acARDAkevyRh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e82b713-b57e-4bc7-87a3-a1c157a50083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98732/98732 [00:18<00:00, 5385.95it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sou_vocab.set_default_index(sou_vocab[\"<unk>\"])\n",
        "tar_vocab.set_default_index(tar_vocab[\"<unk>\"])\n",
        "print(f\"Size of Source Vocab : {len(sou_vocab)}\\n Size of Target Vocab : {len(tar_vocab)}\")\n",
        "\n",
        "text_transform_tar = lambda x: [tar_vocab['<sos>']] + [tar_vocab[token.lower()] for token in tokenizer_eng(x)] + [tar_vocab['<eos>']]\n",
        "text_transform_sou = lambda x: [sou_vocab['<sos>']] + [sou_vocab[token.lower()] for token in tokenizer_eng(x)] + [sou_vocab['<eos>']]\n",
        "\n",
        "def collate_batch(batch):\n",
        "    src_list, tgt_list = [], []\n",
        "    for data in batch:\n",
        "        src_list.append(torch.tensor(text_transform_sou(data['text'])))\n",
        "        tgt_list.append(torch.tensor(text_transform_tar(data['label'])))\n",
        "\n",
        "    src_list = pad_sequence(src_list, padding_value=sou_vocab['<pad>']).T\n",
        "    tgt_list = pad_sequence(tgt_list, padding_value=tar_vocab['<pad>']).T\n",
        "\n",
        "    inp = {\n",
        "        \"src\": src_list,\n",
        "        \"tgt\": tgt_list\n",
        "    }\n",
        "\n",
        "    return inp\n",
        "\n",
        "\n",
        "num_epochs = 30\n",
        "batch_size = 16\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 0.001\n",
        "writer = SummaryWriter(f\"runs/loss\")\n",
        "\n",
        "train_dataloader = DataLoader(train,\n",
        "                              collate_fn=collate_batch,\n",
        "                              shuffle=True,\n",
        "                              batch_size=batch_size,\n",
        "                              pin_memory=True)\n",
        "test_dataloader = DataLoader(test,\n",
        "                              collate_fn=collate_batch,\n",
        "                              shuffle=False,\n",
        "                              batch_size=batch_size,\n",
        "                              pin_memory=True)"
      ],
      "metadata": {
        "id": "V74d3BjHv3hK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82f14d17-995f-4619-bea7-712ba514b573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of Source Vocab : 8112\n",
            " Size of Target Vocab : 5437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "76C137YB2bPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "transformer_model = Transformer(embed_dim=512,\n",
        "                                src_vocab_size=len(sou_vocab),\n",
        "                                target_vocab_size=len(tar_vocab),\n",
        "                                max_seq_length=50,\n",
        "                                num_layers=6,\n",
        "                                expansion_factor=4,\n",
        "                                n_heads=8,\n",
        "                                device=device)\n",
        "transformer_model.src_pad_idx = sou_vocab['<pad>']\n",
        "transformer_model.tgt_pad_idx = tar_vocab['<pad>']\n",
        "\n",
        "import math\n",
        "total_steps = num_epochs*math.ceil(len(train)/batch_size)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
        "                                               max_lr=learning_rate,\n",
        "                                               total_steps=total_steps,\n",
        "                                               pct_start=0.33,\n",
        "                                               div_factor=1e3,\n",
        "                                               final_div_factor=1e2)\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tar_vocab['<pad>'])"
      ],
      "metadata": {
        "id": "DJ47QLXvvuTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_model = transformer_model.to(device)"
      ],
      "metadata": {
        "id": "_Utmk86k2fWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_seq_beam_search(model, src, device, k=2, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    src_mask = model.make_pad_mask(src, model.src_pad_idx)\n",
        "    with torch.no_grad():\n",
        "        enc_out = model.encoder(src, src_mask)\n",
        "\n",
        "    # beam search\n",
        "\n",
        "    candidates = [(torch.LongTensor([tar_vocab['<sos>']]), 0.0)]\n",
        "\n",
        "    final_translations = []\n",
        "\n",
        "    for a in range(max_len):\n",
        "\n",
        "        input_batch = torch.concat([c[0].unsqueeze(0) for c in candidates], dim=0).to(device)\n",
        "\n",
        "        if a>0:\n",
        "            enc_out_repeat = enc_out.repeat(input_batch.shape[0], 1, 1)\n",
        "        else:\n",
        "            enc_out_repeat = enc_out\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.decoder(input_batch, enc_out_repeat, model.make_tgt_mask(input_batch), src_mask).detach().cpu()\n",
        "        output[:, :, :2] = float(\"-1e20\")\n",
        "        output = output[:, -1, :]\n",
        "        output = F.log_softmax(output, dim=-1)\n",
        "\n",
        "\n",
        "        topk_output = torch.topk(output, k, dim=-1)\n",
        "        topk_tokens = topk_output.indices\n",
        "        topk_scores = topk_output.values\n",
        "\n",
        "\n",
        "        new_seq = torch.concat([torch.concat([torch.vstack([c[0] for _ in range(k)]), topk_tokens[i].reshape(-1,1)], dim=-1) for i,c in enumerate(candidates)], dim=0)\n",
        "        new_scores = torch.concat([c[1] + topk_scores[i] for i,c in enumerate(candidates)], dim=0)\n",
        "\n",
        "\n",
        "        topk_new = torch.topk(new_scores, k=k).indices.tolist()\n",
        "\n",
        "        new_candidates = []\n",
        "\n",
        "        for i in range(k):\n",
        "            if new_seq[topk_new[i]][-1] == tar_vocab[\"<eos>\"] or a==max_len-1:\n",
        "                final_translations.append((new_seq[topk_new[i]].tolist(), int(new_scores[topk_new[i]])))\n",
        "            else:\n",
        "                new_candidate = (new_seq[topk_new[i]], new_scores[topk_new[i]])\n",
        "                new_candidates.append(new_candidate)\n",
        "\n",
        "\n",
        "        if len(new_candidates) > 0:\n",
        "            candidates = new_candidates\n",
        "        else:\n",
        "            break\n",
        "\n",
        "\n",
        "    return final_translations\n",
        "def translate_seq(model, src, device, max_len=50):\n",
        "    model.eval()\n",
        "    src_mask = model.make_pad_mask(src, model.src_pad_idx)\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src, src_mask)\n",
        "    tgt_indexes = [tar_vocab[\"<sos>\"]]\n",
        "    for i in range(max_len):\n",
        "        tgt_tensor = torch.LongTensor(tgt_indexes).unsqueeze(0).to(device)\n",
        "        tgt_mask = model.make_tgt_mask(tgt_tensor)\n",
        "        with torch.no_grad():\n",
        "            output = model.decoder(tgt_tensor, enc_src, tgt_mask, src_mask)\n",
        "        output[:, :, :2] = float(\"-1e20\")  # cannot predict <unk>, <pad> token\n",
        "        output = output[:, -1, :] # pick the last token\n",
        "        output = F.softmax(output, dim=-1)\n",
        "        pred_token = output.argmax(-1).item()\n",
        "        tgt_indexes.append(pred_token)\n",
        "        if pred_token == tar_vocab[\"<eos>\"]:\n",
        "            break\n",
        "    return tgt_indexes\n",
        "\n",
        "\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "class AvgMeter:\n",
        "    def __init__(self, name=\"Metric\"):\n",
        "        self.name = name\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.avg, self.sum, self.count = [0]*3\n",
        "\n",
        "    def update(self, val, count=1):\n",
        "        self.count += count\n",
        "        self.sum += val * count\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __repr__(self):\n",
        "        text = f\"{self.name}: {self.avg:.4f}\"\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "N_p9mmMfQuMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twBrTZU7D_JV",
        "outputId": "556db1ce-dad2-4a48-f48a-df586432b318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1 / 30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6171/6171 [06:18<00:00, 16.32it/s, loss=5.14, lr=2.59e-5, step=6171]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example source data: \n",
            "scar lower\n",
            "\n",
            "Original SNOMED mapping : \n",
            "scar\n",
            "\n",
            "Generated by model SNOMED mapping : \n",
            "neoplasm of skin\n",
            "\n",
            "[Epoch 2 / 30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6171/6171 [05:58<00:00, 17.22it/s, loss=3.13, lr=9.83e-5, step=12342]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example source data: \n",
            "angiosarcoma\n",
            "\n",
            "Original SNOMED mapping : \n",
            "angiosarcoma\n",
            "\n",
            "Generated by model SNOMED mapping : \n",
            "neoplasm of lung\n",
            "\n",
            "[Epoch 3 / 30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6171/6171 [06:05<00:00, 16.90it/s, loss=1.91, lr=0.000211, step=18513]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example source data: \n",
            "lower nose invasive basal cell carcinoma\n",
            "\n",
            "Original SNOMED mapping : \n",
            "neoplasm of nose\n",
            "\n",
            "Generated by model SNOMED mapping : \n",
            "neoplasm of nose\n",
            "\n",
            "[Epoch 4 / 30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6171/6171 [06:08<00:00, 16.75it/s, loss=1.45, lr=0.000352, step=24684]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example source data: \n",
            "diabetes improved\n",
            "\n",
            "Original SNOMED mapping : \n",
            "diabetes mellitus\n",
            "\n",
            "Generated by model SNOMED mapping : \n",
            "type 1 diabetes mellitus\n",
            "\n",
            "[Epoch 5 / 30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6171/6171 [06:10<00:00, 16.64it/s, loss=1.28, lr=0.000508, step=30855]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example source data: \n",
            "fundic gland polyps of stomach benign\n",
            "\n",
            "Original SNOMED mapping : \n",
            "benign neoplasm of stomach\n",
            "\n",
            "Generated by model SNOMED mapping : \n",
            "benign neoplasm of stomach\n",
            "\n",
            "[Epoch 6 / 30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6171/6171 [06:05<00:00, 16.86it/s, loss=1.21, lr=0.000664, step=37026]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example source data: \n",
            "cyst of skin\n",
            "\n",
            "Original SNOMED mapping : \n",
            "cyst of skin\n",
            "\n",
            "Generated by model SNOMED mapping : \n",
            "cyst of skin\n",
            "\n",
            "[Epoch 7 / 30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6171/6171 [06:08<00:00, 16.74it/s, loss=1.16, lr=0.000803, step=43197]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example source data: \n",
            "invasive intraductal carcinoma\n",
            "\n",
            "Original SNOMED mapping : \n",
            "neoplastic disease\n",
            "\n",
            "Generated by model SNOMED mapping : \n",
            "neoplastic disease\n",
            "\n",
            "[Epoch 8 / 30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6171/6171 [06:03<00:00, 16.99it/s, loss=1.13, lr=0.000912, step=49368]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example source data: \n",
            "pericarditis secondary to uremia\n",
            "\n",
            "Original SNOMED mapping : \n",
            "pericarditis secondary to uremia\n",
            "\n",
            "Generated by model SNOMED mapping : \n",
            "secondary pericarditis\n",
            "\n",
            "[Epoch 9 / 30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6171/6171 [06:00<00:00, 17.13it/s, loss=1.08, lr=0.00098, step=55539]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example source data: \n",
            "kidney disease no\n",
            "\n",
            "Original SNOMED mapping : \n",
            "disorder of kidney and/or ureter\n",
            "\n",
            "Generated by model SNOMED mapping : \n",
            "disorder of the ankle and/or foot\n",
            "\n",
            "[Epoch 10 / 30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6171/6171 [05:59<00:00, 17.15it/s, loss=1.05, lr=0.001, step=61710]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example source data: \n",
            "urethral discharge\n",
            "\n",
            "Original SNOMED mapping : \n",
            "urethral discharge\n",
            "\n",
            "Generated by model SNOMED mapping : \n",
            "urethral discharge\n",
            "\n",
            "[Epoch 11 / 30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▍      | 2147/6171 [02:04<03:48, 17.58it/s, loss=0.933, lr=0.000999, step=63857]"
          ]
        }
      ],
      "source": [
        "### training ###\n",
        "step = 0\n",
        "for epoch in range(1, num_epochs+1):\n",
        "\n",
        "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "\n",
        "    checkpoint = {\"state_dict\": transformer_model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
        "    torch.save(checkpoint, \"/content/drive/MyDrive/internal/mpng_cust/my_checkpoint.pth.tar\")\n",
        "\n",
        "    loss_meter = AvgMeter()\n",
        "    transformer_model.train()\n",
        "\n",
        "    bar = tqdm(train_dataloader, total=math.ceil(len(train)/batch_size))\n",
        "\n",
        "    for idx, data in enumerate(bar):\n",
        "\n",
        "        source_ = data[\"src\"].to(device)\n",
        "        target_ = data[\"tgt\"].to(device)\n",
        "\n",
        "        count = source_.shape[0]\n",
        "\n",
        "        output = transformer_model(source_, target_[:,:-1])\n",
        "\n",
        "        output = output.reshape(-1, output.shape[2])\n",
        "        target_ = target_[:, 1:]\n",
        "        target_ = target_.reshape(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, target_)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), max_norm=1)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
        "        step += 1\n",
        "\n",
        "        loss_meter.update(loss.item(), count)\n",
        "        bar.set_postfix(loss=loss_meter.avg, lr=get_lr(optimizer), step=step)\n",
        "\n",
        "    # Example Generation (Greedy Decode)\n",
        "    ex = test[random.randint(0, len(test))]\n",
        "    sentence = ex['text']\n",
        "    src_indexes = torch.tensor(text_transform_sou(sentence)).unsqueeze(0).to(device)\n",
        "    mapped_sentence_idx = translate_seq(transformer_model, src_indexes, device=device, max_len=30)\n",
        "    mapped_sentence = [tar_vocab.get_itos()[i] for i in mapped_sentence_idx]\n",
        "    print(f\"\\nExample source data: \\n{sentence}\\n\")\n",
        "    print(f\"Original SNOMED mapping : \\n{ex['label']}\\n\" )\n",
        "    print(f\"Generated by model SNOMED mapping : \\n{' '.join(mapped_sentence[1:-1])}\\n\")\n",
        "\n",
        "    del src_indexes, ex, sentence, mapped_sentence_idx, mapped_sentence, checkpoint\n",
        "    torch.cuda.empty_cache()\n",
        "    _ = gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu(data, model, device, max_len=50):\n",
        "    tgts = []\n",
        "    preds = []\n",
        "    for datum in tqdm(data):\n",
        "        src = datum[\"text\"]\n",
        "        tgt = datum[\"label\"]\n",
        "        src_idx = torch.tensor(text_transform_sou(src)).unsqueeze(0).to(device)\n",
        "        pred_tgt = translate_seq(model, src_idx, device, max_len)\n",
        "        pred_tgt = pred_tgt[1:-1]\n",
        "        pred_sent = [tar_vocab.get_itos()[i] for i in pred_tgt]\n",
        "        preds.append(pred_sent)\n",
        "        tgts.append([tokenizer_eng(tgt.lower())])\n",
        "    return bleu_score(preds, tgts)"
      ],
      "metadata": {
        "id": "EU5e8TrgbTYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = calculate_bleu(test, transformer_model, device)\n",
        "print(\"BLEU Score Achieved :\", bleu)"
      ],
      "metadata": {
        "id": "pV6iI_yWbheL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69b65636-9a38-4936-941e-4421ec67f75e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17424/17424 [10:46<00:00, 26.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score Achieved : 0.6227594614028931\n"
          ]
        }
      ]
    }
  ]
}